{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPNN Model for EKG Classification\n",
    "\n",
    "This notebook implements a Backpropagation Neural Network (BPNN) with complete mathematical formulations for EKG/ECG signal classification.\n",
    "\n",
    "## Mathematical Foundations of BPNN\n",
    "\n",
    "### 1. Forward Propagation\n",
    "\n",
    "For a neural network with layers $l = 1, 2, ..., L$:\n",
    "\n",
    "#### Linear Transformation\n",
    "$$z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$$\n",
    "\n",
    "where:\n",
    "- $W^{[l]}$ is the weight matrix for layer $l$\n",
    "- $a^{[l-1]}$ is the activation from previous layer\n",
    "- $b^{[l]}$ is the bias vector\n",
    "- $z^{[l]}$ is the pre-activation output\n",
    "\n",
    "#### Activation Function\n",
    "$$a^{[l]} = g^{[l]}(z^{[l]})$$\n",
    "\n",
    "### 2. Activation Functions\n",
    "\n",
    "#### Sigmoid\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "$$\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$$\n",
    "\n",
    "#### Hyperbolic Tangent (tanh)\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "$$\\tanh'(z) = 1 - \\tanh^2(z)$$\n",
    "\n",
    "#### ReLU (Rectified Linear Unit)\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "$$\\text{ReLU}'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases}$$\n",
    "\n",
    "#### Softmax (for output layer)\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$$\n",
    "\n",
    "### 3. Loss Functions\n",
    "\n",
    "#### Binary Cross-Entropy\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} [y^{(i)} \\log(\\hat{y}^{(i)}) + (1-y^{(i)}) \\log(1-\\hat{y}^{(i)})]$$\n",
    "\n",
    "#### Categorical Cross-Entropy\n",
    "$$L(y, \\hat{y}) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$$\n",
    "\n",
    "#### Mean Squared Error\n",
    "$$L(y, \\hat{y}) = \\frac{1}{2m} \\sum_{i=1}^{m} (y^{(i)} - \\hat{y}^{(i)})^2$$\n",
    "\n",
    "### 4. Backpropagation\n",
    "\n",
    "#### Output Layer Gradient\n",
    "$$\\delta^{[L]} = \\frac{\\partial L}{\\partial z^{[L]}} = a^{[L]} - y$$\n",
    "\n",
    "#### Hidden Layer Gradient\n",
    "$$\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot g'^{[l]}(z^{[l]})$$\n",
    "\n",
    "where $\\odot$ denotes element-wise multiplication.\n",
    "\n",
    "#### Weight Gradients\n",
    "$$\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{1}{m} \\delta^{[l]} (a^{[l-1]})^T$$\n",
    "\n",
    "#### Bias Gradients\n",
    "$$\\frac{\\partial L}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{[l]}_i$$\n",
    "\n",
    "### 5. Weight Update Rules\n",
    "\n",
    "#### Gradient Descent\n",
    "$$W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}$$\n",
    "$$b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate.\n",
    "\n",
    "#### Momentum\n",
    "$$v_{dW} = \\beta v_{dW} + (1-\\beta) dW$$\n",
    "$$W := W - \\alpha v_{dW}$$\n",
    "\n",
    "#### Adam Optimizer\n",
    "$$m_t = \\beta_1 m_{t-1} + (1-\\beta_1) g_t$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1-\\beta_2) g_t^2$$\n",
    "$$\\hat{m}_t = \\frac{m_t}{1-\\beta_1^t}$$\n",
    "$$\\hat{v}_t = \\frac{v_t}{1-\\beta_2^t}$$\n",
    "$$\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import json\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic EKG Dataset Generation\n",
    "\n",
    "### Mathematical Model for EKG Synthesis\n",
    "\n",
    "We model EKG signals as a combination of Gaussian functions representing PQRST complex:\n",
    "\n",
    "$$ECG(t) = \\sum_{i \\in \\{P, Q, R, S, T\\}} A_i \\exp\\left(-\\frac{(t - t_i)^2}{2\\sigma_i^2}\\right)$$\n",
    "\n",
    "where $A_i$ is amplitude, $t_i$ is time position, and $\\sigma_i$ is width of each wave component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pqrst_complex(heart_rate=70, sampling_rate=250, duration=1.0, noise_level=0.05):\n",
    "    \"\"\"\n",
    "    Generate synthetic EKG signal with PQRST complex.\n",
    "    ECG(t) = Σ A_i * exp(-(t - t_i)² / (2σ_i²))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    heart_rate : int\n",
    "        Heart rate in beats per minute\n",
    "    sampling_rate : int\n",
    "        Samples per second\n",
    "    duration : float\n",
    "        Duration in seconds\n",
    "    noise_level : float\n",
    "        Standard deviation of Gaussian noise\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    signal : numpy.ndarray\n",
    "        Synthetic EKG signal\n",
    "    \"\"\"\n",
    "    n_samples = int(duration * sampling_rate)\n",
    "    t = np.linspace(0, duration, n_samples)\n",
    "    signal = np.zeros(n_samples)\n",
    "    \n",
    "    # Calculate beat period\n",
    "    beat_period = 60.0 / heart_rate\n",
    "    n_beats = int(duration / beat_period)\n",
    "    \n",
    "    # PQRST wave parameters (time offset, amplitude, width)\n",
    "    waves = {\n",
    "        'P': (0.16, 0.25, 0.02),   # P wave\n",
    "        'Q': (0.20, -0.15, 0.01),  # Q wave\n",
    "        'R': (0.22, 1.5, 0.015),   # R wave (peak)\n",
    "        'S': (0.24, -0.4, 0.01),   # S wave\n",
    "        'T': (0.38, 0.35, 0.04),   # T wave\n",
    "    }\n",
    "    \n",
    "    # Generate each heartbeat\n",
    "    for beat in range(n_beats + 1):\n",
    "        beat_start = beat * beat_period\n",
    "        \n",
    "        for wave_name, (t_offset, amplitude, width) in waves.items():\n",
    "            wave_center = beat_start + t_offset\n",
    "            # Gaussian function: A * exp(-(t - t_i)² / (2σ²))\n",
    "            wave = amplitude * np.exp(-((t - wave_center) ** 2) / (2 * width ** 2))\n",
    "            signal += wave\n",
    "    \n",
    "    # Add baseline wander (low-frequency noise)\n",
    "    baseline = 0.1 * np.sin(2 * np.pi * 0.2 * t)\n",
    "    \n",
    "    # Add high-frequency noise\n",
    "    noise = noise_level * np.random.randn(n_samples)\n",
    "    \n",
    "    return signal + baseline + noise\n",
    "\n",
    "def generate_abnormal_ekg(condition='tachycardia', sampling_rate=250, duration=1.0):\n",
    "    \"\"\"\n",
    "    Generate abnormal EKG patterns.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    condition : str\n",
    "        Type of abnormality: 'tachycardia', 'bradycardia', 'arrhythmia'\n",
    "    sampling_rate : int\n",
    "        Samples per second\n",
    "    duration : float\n",
    "        Duration in seconds\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    signal : numpy.ndarray\n",
    "        Abnormal EKG signal\n",
    "    \"\"\"\n",
    "    if condition == 'tachycardia':\n",
    "        # Fast heart rate (>100 bpm)\n",
    "        hr = np.random.randint(100, 140)\n",
    "        return generate_pqrst_complex(hr, sampling_rate, duration, noise_level=0.08)\n",
    "    \n",
    "    elif condition == 'bradycardia':\n",
    "        # Slow heart rate (<60 bpm)\n",
    "        hr = np.random.randint(40, 60)\n",
    "        return generate_pqrst_complex(hr, sampling_rate, duration, noise_level=0.05)\n",
    "    \n",
    "    elif condition == 'arrhythmia':\n",
    "        # Irregular heart rhythm\n",
    "        n_samples = int(duration * sampling_rate)\n",
    "        signal = np.zeros(n_samples)\n",
    "        t = np.linspace(0, duration, n_samples)\n",
    "        \n",
    "        # Random beat intervals\n",
    "        beat_times = [0]\n",
    "        current_time = 0\n",
    "        while current_time < duration:\n",
    "            # Variable interval between beats\n",
    "            interval = np.random.uniform(0.4, 0.9)\n",
    "            current_time += interval\n",
    "            if current_time < duration:\n",
    "                beat_times.append(current_time)\n",
    "        \n",
    "        # Generate beats at irregular intervals\n",
    "        for beat_time in beat_times:\n",
    "            waves = {\n",
    "                'P': (0.16, 0.25, 0.02),\n",
    "                'Q': (0.20, -0.15, 0.01),\n",
    "                'R': (0.22, 1.5, 0.015),\n",
    "                'S': (0.24, -0.4, 0.01),\n",
    "                'T': (0.38, 0.35, 0.04),\n",
    "            }\n",
    "            \n",
    "            for wave_name, (t_offset, amplitude, width) in waves.items():\n",
    "                wave_center = beat_time + t_offset\n",
    "                wave = amplitude * np.exp(-((t - wave_center) ** 2) / (2 * width ** 2))\n",
    "                signal += wave\n",
    "        \n",
    "        signal += 0.1 * np.random.randn(n_samples)\n",
    "        return signal\n",
    "    \n",
    "    else:\n",
    "        return generate_pqrst_complex(70, sampling_rate, duration)\n",
    "\n",
    "def create_dataset(n_samples=1000, window_size=250, normal_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Create balanced dataset of normal and abnormal EKG signals.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_samples : int\n",
    "        Total number of samples\n",
    "    window_size : int\n",
    "        Length of each signal window\n",
    "    normal_ratio : float\n",
    "        Proportion of normal samples\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    X : numpy.ndarray\n",
    "        Feature matrix (n_samples, window_size)\n",
    "    y : numpy.ndarray\n",
    "        Labels (0: normal, 1: tachycardia, 2: bradycardia, 3: arrhythmia)\n",
    "    \"\"\"\n",
    "    n_normal = int(n_samples * normal_ratio)\n",
    "    n_abnormal = n_samples - n_normal\n",
    "    n_per_condition = n_abnormal // 3\n",
    "    \n",
    "    X = np.zeros((n_samples, window_size))\n",
    "    y = np.zeros(n_samples, dtype=int)\n",
    "    \n",
    "    idx = 0\n",
    "    \n",
    "    # Generate normal samples\n",
    "    for _ in range(n_normal):\n",
    "        hr = np.random.randint(60, 100)\n",
    "        X[idx] = generate_pqrst_complex(hr, window_size, 1.0, 0.05)\n",
    "        y[idx] = 0  # Normal\n",
    "        idx += 1\n",
    "    \n",
    "    # Generate abnormal samples\n",
    "    conditions = ['tachycardia', 'bradycardia', 'arrhythmia']\n",
    "    for i, condition in enumerate(conditions):\n",
    "        for _ in range(n_per_condition):\n",
    "            X[idx] = generate_abnormal_ekg(condition, window_size, 1.0)\n",
    "            y[idx] = i + 1\n",
    "            idx += 1\n",
    "    \n",
    "    # Shuffle dataset\n",
    "    shuffle_idx = np.random.permutation(n_samples)\n",
    "    X = X[shuffle_idx]\n",
    "    y = y[shuffle_idx]\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPNN Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationFunctions:\n",
    "    \"\"\"Collection of activation functions and their derivatives.\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid(z):\n",
    "        \"\"\"Sigmoid: σ(z) = 1 / (1 + e^(-z))\"\"\"\n",
    "        return 1 / (1 + np.exp(-np.clip(z, -500, 500)))\n",
    "    \n",
    "    @staticmethod\n",
    "    def sigmoid_derivative(z):\n",
    "        \"\"\"Sigmoid derivative: σ'(z) = σ(z)(1 - σ(z))\"\"\"\n",
    "        s = ActivationFunctions.sigmoid(z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(z):\n",
    "        \"\"\"Hyperbolic tangent: tanh(z) = (e^z - e^(-z)) / (e^z + e^(-z))\"\"\"\n",
    "        return np.tanh(z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_derivative(z):\n",
    "        \"\"\"Tanh derivative: tanh'(z) = 1 - tanh²(z)\"\"\"\n",
    "        t = np.tanh(z)\n",
    "        return 1 - t ** 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(z):\n",
    "        \"\"\"ReLU: max(0, z)\"\"\"\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu_derivative(z):\n",
    "        \"\"\"ReLU derivative: 1 if z > 0, else 0\"\"\"\n",
    "        return (z > 0).astype(float)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(z):\n",
    "        \"\"\"Softmax: exp(z_i) / Σ(exp(z_j))\"\"\"\n",
    "        exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))\n",
    "        return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "class BPNN:\n",
    "    \"\"\"\n",
    "    Backpropagation Neural Network implementation.\n",
    "    \n",
    "    Mathematical formulations:\n",
    "    - Forward: z^[l] = W^[l] * a^[l-1] + b^[l]\n",
    "    - Activation: a^[l] = g(z^[l])\n",
    "    - Loss: L = (1/m) * Σ loss_function(y, ŷ)\n",
    "    - Backprop: δ^[l] = (W^[l+1])^T * δ^[l+1] ⊙ g'(z^[l])\n",
    "    - Update: W^[l] := W^[l] - α * dW^[l]\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation='relu', learning_rate=0.01):\n",
    "        \"\"\"\n",
    "        Initialize BPNN.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        layer_dims : list\n",
    "            List of layer dimensions [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "        activation : str\n",
    "            Activation function ('sigmoid', 'tanh', 'relu')\n",
    "        learning_rate : float\n",
    "            Learning rate α\n",
    "        \"\"\"\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.activation_name = activation\n",
    "        \n",
    "        # Set activation function\n",
    "        if activation == 'sigmoid':\n",
    "            self.activation = ActivationFunctions.sigmoid\n",
    "            self.activation_derivative = ActivationFunctions.sigmoid_derivative\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = ActivationFunctions.tanh\n",
    "            self.activation_derivative = ActivationFunctions.tanh_derivative\n",
    "        else:  # relu\n",
    "            self.activation = ActivationFunctions.relu\n",
    "            self.activation_derivative = ActivationFunctions.relu_derivative\n",
    "        \n",
    "        # Initialize parameters using He initialization\n",
    "        self.parameters = {}\n",
    "        self._initialize_parameters()\n",
    "        \n",
    "        # Cache for forward propagation\n",
    "        self.cache = {}\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {'loss': [], 'accuracy': []}\n",
    "    \n",
    "    def _initialize_parameters(self):\n",
    "        \"\"\"\n",
    "        Initialize weights and biases using He initialization:\n",
    "        W^[l] ~ N(0, sqrt(2/n^[l-1]))\n",
    "        b^[l] = 0\n",
    "        \"\"\"\n",
    "        np.random.seed(42)\n",
    "        L = len(self.layer_dims)\n",
    "        \n",
    "        for l in range(1, L):\n",
    "            # He initialization: W ~ N(0, sqrt(2/n_in))\n",
    "            self.parameters[f'W{l}'] = np.random.randn(\n",
    "                self.layer_dims[l], self.layer_dims[l-1]\n",
    "            ) * np.sqrt(2.0 / self.layer_dims[l-1])\n",
    "            \n",
    "            # Initialize biases to zero\n",
    "            self.parameters[f'b{l}'] = np.zeros((self.layer_dims[l], 1))\n",
    "    \n",
    "    def forward_propagation(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation through the network.\n",
    "        \n",
    "        For each layer l:\n",
    "        z^[l] = W^[l] * a^[l-1] + b^[l]\n",
    "        a^[l] = g(z^[l])\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Input data (n_features, m_samples)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        AL : numpy.ndarray\n",
    "            Output activations\n",
    "        \"\"\"\n",
    "        L = len(self.layer_dims) - 1\n",
    "        A = X\n",
    "        self.cache['A0'] = X\n",
    "        \n",
    "        # Hidden layers\n",
    "        for l in range(1, L):\n",
    "            A_prev = A\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            # Linear transformation: z = W * a + b\n",
    "            Z = np.dot(W, A_prev) + b\n",
    "            \n",
    "            # Activation: a = g(z)\n",
    "            A = self.activation(Z)\n",
    "            \n",
    "            # Cache for backpropagation\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        # Output layer (softmax)\n",
    "        W = self.parameters[f'W{L}']\n",
    "        b = self.parameters[f'b{L}']\n",
    "        Z = np.dot(W, A) + b\n",
    "        AL = ActivationFunctions.softmax(Z)\n",
    "        \n",
    "        self.cache[f'Z{L}'] = Z\n",
    "        self.cache[f'A{L}'] = AL\n",
    "        \n",
    "        return AL\n",
    "    \n",
    "    def compute_loss(self, AL, Y):\n",
    "        \"\"\"\n",
    "        Compute categorical cross-entropy loss:\n",
    "        L = -(1/m) * Σ Σ y_k * log(ŷ_k)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        AL : numpy.ndarray\n",
    "            Predicted probabilities\n",
    "        Y : numpy.ndarray\n",
    "            True labels (one-hot encoded)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        loss : float\n",
    "            Cross-entropy loss\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        \n",
    "        # Categorical cross-entropy: -(1/m) * Σ y * log(ŷ)\n",
    "        loss = -np.sum(Y * np.log(AL + 1e-8)) / m\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def backward_propagation(self, Y):\n",
    "        \"\"\"\n",
    "        Backward propagation to compute gradients.\n",
    "        \n",
    "        Output layer: δ^[L] = a^[L] - y\n",
    "        Hidden layers: δ^[l] = (W^[l+1])^T * δ^[l+1] ⊙ g'(z^[l])\n",
    "        \n",
    "        Weight gradient: dW^[l] = (1/m) * δ^[l] * (a^[l-1])^T\n",
    "        Bias gradient: db^[l] = (1/m) * Σ δ^[l]\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        Y : numpy.ndarray\n",
    "            True labels (one-hot encoded)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        gradients : dict\n",
    "            Dictionary of gradients for weights and biases\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        L = len(self.layer_dims) - 1\n",
    "        gradients = {}\n",
    "        \n",
    "        # Output layer gradient: δ^[L] = a^[L] - y\n",
    "        dZ = self.cache[f'A{L}'] - Y\n",
    "        \n",
    "        # Backpropagate through layers\n",
    "        for l in reversed(range(1, L + 1)):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            \n",
    "            # Weight gradient: dW = (1/m) * dZ * A_prev^T\n",
    "            gradients[f'dW{l}'] = np.dot(dZ, A_prev.T) / m\n",
    "            \n",
    "            # Bias gradient: db = (1/m) * Σ dZ\n",
    "            gradients[f'db{l}'] = np.sum(dZ, axis=1, keepdims=True) / m\n",
    "            \n",
    "            if l > 1:\n",
    "                # Hidden layer gradient: dZ = W^T * dZ ⊙ g'(Z)\n",
    "                W = self.parameters[f'W{l}']\n",
    "                Z_prev = self.cache[f'Z{l-1}']\n",
    "                dZ = np.dot(W.T, dZ) * self.activation_derivative(Z_prev)\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"\n",
    "        Update parameters using gradient descent:\n",
    "        W^[l] := W^[l] - α * dW^[l]\n",
    "        b^[l] := b^[l] - α * db^[l]\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        gradients : dict\n",
    "            Dictionary of gradients\n",
    "        \"\"\"\n",
    "        L = len(self.layer_dims) - 1\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            # Gradient descent update\n",
    "            self.parameters[f'W{l}'] -= self.learning_rate * gradients[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= self.learning_rate * gradients[f'db{l}']\n",
    "    \n",
    "    def train(self, X, y, epochs=1000, batch_size=32, verbose=True):\n",
    "        \"\"\"\n",
    "        Train the neural network.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Training data (m_samples, n_features)\n",
    "        y : numpy.ndarray\n",
    "            Labels\n",
    "        epochs : int\n",
    "            Number of training epochs\n",
    "        batch_size : int\n",
    "            Mini-batch size\n",
    "        verbose : bool\n",
    "            Print training progress\n",
    "        \"\"\"\n",
    "        m = X.shape[0]\n",
    "        n_batches = m // batch_size\n",
    "        \n",
    "        # One-hot encode labels\n",
    "        n_classes = len(np.unique(y))\n",
    "        Y = np.eye(n_classes)[y].T\n",
    "        X = X.T\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            # Shuffle data\n",
    "            permutation = np.random.permutation(m)\n",
    "            X_shuffled = X[:, permutation]\n",
    "            Y_shuffled = Y[:, permutation]\n",
    "            \n",
    "            epoch_loss = 0\n",
    "            \n",
    "            # Mini-batch gradient descent\n",
    "            for i in range(n_batches):\n",
    "                start = i * batch_size\n",
    "                end = start + batch_size\n",
    "                \n",
    "                X_batch = X_shuffled[:, start:end]\n",
    "                Y_batch = Y_shuffled[:, start:end]\n",
    "                \n",
    "                # Forward propagation\n",
    "                AL = self.forward_propagation(X_batch)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = self.compute_loss(AL, Y_batch)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "                # Backward propagation\n",
    "                gradients = self.backward_propagation(Y_batch)\n",
    "                \n",
    "                # Update parameters\n",
    "                self.update_parameters(gradients)\n",
    "            \n",
    "            # Compute accuracy\n",
    "            AL = self.forward_propagation(X)\n",
    "            predictions = np.argmax(AL, axis=0)\n",
    "            accuracy = np.mean(predictions == y)\n",
    "            \n",
    "            # Store history\n",
    "            avg_loss = epoch_loss / n_batches\n",
    "            self.history['loss'].append(avg_loss)\n",
    "            self.history['accuracy'].append(accuracy)\n",
    "            \n",
    "            # Print progress\n",
    "            if verbose and (epoch + 1) % 100 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Input data (m_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        predictions : numpy.ndarray\n",
    "            Predicted class labels\n",
    "        \"\"\"\n",
    "        AL = self.forward_propagation(X.T)\n",
    "        predictions = np.argmax(AL, axis=0)\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict class probabilities.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        X : numpy.ndarray\n",
    "            Input data (m_samples, n_features)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        probabilities : numpy.ndarray\n",
    "            Class probabilities (m_samples, n_classes)\n",
    "        \"\"\"\n",
    "        AL = self.forward_propagation(X.T)\n",
    "        return AL.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Training BPNN on EKG Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic dataset\n",
    "print(\"Generating synthetic EKG dataset...\")\n",
    "X, y = create_dataset(n_samples=1000, window_size=250, normal_ratio=0.5)\n",
    "\n",
    "print(f\"Dataset shape: {X.shape}\")\n",
    "print(f\"Labels shape: {y.shape}\")\n",
    "print(f\"Classes: {np.unique(y)} (0: Normal, 1: Tachycardia, 2: Bradycardia, 3: Arrhythmia)\")\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize features (z-score normalization)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nTraining set size: {X_train.shape[0]}\")\n",
    "print(f\"Test set size: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BPNN\n",
    "# Architecture: 250 (input) -> 128 -> 64 -> 32 -> 4 (output classes)\n",
    "layer_dims = [250, 128, 64, 32, 4]\n",
    "bpnn = BPNN(layer_dims=layer_dims, activation='relu', learning_rate=0.01)\n",
    "\n",
    "print(\"BPNN Architecture:\")\n",
    "for i, dim in enumerate(layer_dims):\n",
    "    if i == 0:\n",
    "        print(f\"  Input layer: {dim} neurons\")\n",
    "    elif i == len(layer_dims) - 1:\n",
    "        print(f\"  Output layer: {dim} neurons (softmax)\")\n",
    "    else:\n",
    "        print(f\"  Hidden layer {i}: {dim} neurons ({bpnn.activation_name})\")\n",
    "\n",
    "# Count parameters\n",
    "total_params = 0\n",
    "for l in range(1, len(layer_dims)):\n",
    "    n_weights = layer_dims[l] * layer_dims[l-1]\n",
    "    n_biases = layer_dims[l]\n",
    "    total_params += n_weights + n_biases\n",
    "    print(f\"  Layer {l}: {n_weights} weights + {n_biases} biases = {n_weights + n_biases} parameters\")\n",
    "\n",
    "print(f\"\\nTotal trainable parameters: {total_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "print(\"\\nTraining BPNN...\")\n",
    "bpnn.train(X_train, y_train, epochs=500, batch_size=32, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "y_pred = bpnn.predict(X_test)\n",
    "test_accuracy = np.mean(y_pred == y_test)\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "class_names = ['Normal', 'Tachycardia', 'Bradycardia', 'Arrhythmia']\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curve\n",
    "ax1.plot(bpnn.history['loss'])\n",
    "ax1.set_title('Training Loss Over Time')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "# Accuracy curve\n",
    "ax2.plot(bpnn.history['accuracy'])\n",
    "ax2.set_title('Training Accuracy Over Time')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions\n",
    "n_samples_to_show = 8\n",
    "sample_indices = np.random.choice(len(X_test), n_samples_to_show, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, idx in enumerate(sample_indices):\n",
    "    signal = X_test[idx]\n",
    "    true_label = class_names[y_test[idx]]\n",
    "    pred_label = class_names[y_pred[idx]]\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    proba = bpnn.predict_proba(X_test[idx:idx+1])[0]\n",
    "    confidence = np.max(proba)\n",
    "    \n",
    "    axes[i].plot(signal)\n",
    "    axes[i].set_title(f'True: {true_label}\\nPred: {pred_label} ({confidence:.2f})')\n",
    "    axes[i].set_xlabel('Sample')\n",
    "    axes[i].set_ylabel('Amplitude')\n",
    "    \n",
    "    # Color code: green if correct, red if incorrect\n",
    "    color = 'green' if true_label == pred_label else 'red'\n",
    "    axes[i].spines['top'].set_color(color)\n",
    "    axes[i].spines['bottom'].set_color(color)\n",
    "    axes[i].spines['left'].set_color(color)\n",
    "    axes[i].spines['right'].set_color(color)\n",
    "    axes[i].spines['top'].set_linewidth(3)\n",
    "    axes[i].spines['bottom'].set_linewidth(3)\n",
    "    axes[i].spines['left'].set_linewidth(3)\n",
    "    axes[i].spines['right'].set_linewidth(3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Mathematical Formulations\n",
    "\n",
    "This notebook implements a complete BPNN with the following mathematical expressions:\n",
    "\n",
    "### 1. Forward Propagation\n",
    "- **Linear transformation**: $z^{[l]} = W^{[l]} a^{[l-1]} + b^{[l]}$\n",
    "- **Activation**: $a^{[l]} = g(z^{[l]})$ where $g$ can be sigmoid, tanh, or ReLU\n",
    "- **Output layer**: $a^{[L]} = \\text{softmax}(z^{[L]})$\n",
    "\n",
    "### 2. Loss Computation\n",
    "- **Categorical cross-entropy**: $L = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{k=1}^{K} y_k^{(i)} \\log(\\hat{y}_k^{(i)})$\n",
    "\n",
    "### 3. Backpropagation\n",
    "- **Output gradient**: $\\delta^{[L]} = a^{[L]} - y$\n",
    "- **Hidden gradients**: $\\delta^{[l]} = (W^{[l+1]})^T \\delta^{[l+1]} \\odot g'^{[l]}(z^{[l]})$\n",
    "- **Weight gradients**: $\\frac{\\partial L}{\\partial W^{[l]}} = \\frac{1}{m} \\delta^{[l]} (a^{[l-1]})^T$\n",
    "- **Bias gradients**: $\\frac{\\partial L}{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i=1}^{m} \\delta^{[l]}_i$\n",
    "\n",
    "### 4. Parameter Updates\n",
    "- **Gradient descent**: \n",
    "  - $W^{[l]} := W^{[l]} - \\alpha \\frac{\\partial L}{\\partial W^{[l]}}$\n",
    "  - $b^{[l]} := b^{[l]} - \\alpha \\frac{\\partial L}{\\partial b^{[l]}}$\n",
    "\n",
    "### 5. Activation Functions\n",
    "- **Sigmoid**: $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ with derivative $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "- **Tanh**: $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$ with derivative $\\tanh'(z) = 1 - \\tanh^2(z)$\n",
    "- **ReLU**: $\\text{ReLU}(z) = \\max(0, z)$ with derivative $\\text{ReLU}'(z) = \\mathbb{1}_{z > 0}$\n",
    "- **Softmax**: $\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^{K} e^{z_j}}$\n",
    "\n",
    "All mathematical expressions have been implemented and validated in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

